---
title: "Katie's Watershed Grabber"
author: "Katie Willi"
date: "2024-12-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE)
```

Load in packages:

```{r}
library(tidyverse)
library(sf)
library(terra)
library(elevatr)
library(dataRetrieval)
library(nhdplusTools)
library(StreamCatTools)
library(tmap)
library(climateR)
library(data.table)
library(mapview)
```

```{r}
abby_raw <- st_read("data/katie/WBD_FINAL_Jan2020_3.shp")

abby <- st_read("data/katie/WBD_FINAL_Jan2020_3.shp") %>%
  rename(site_no = F8) %>%
  mutate(type = ifelse(is.na(site_no), "CO", "USGS"),
         site_no = ifelse(is.na(site_no), usgsstatid, site_no )) %>%
  select(site_no, co_no = dwrabbrev, statname, type)
```

Grab all USGS gages

Using the dataRetrieval package in R, locate all USGS stream gages that measure discharge.

```{r}
states_oi <- c("Colorado", "Nebraska", "Wyoming", "Utah", "New Mexico", "Kansas")

us_sf_object <- tigris::states() %>%
  filter(NAME %in% states_oi)

# Get a list of NWIS sites for all of the states
nwis_sites_by_state <- map(c("CO", "NE", "WY", "UT", "NM", "KS"), #us_sf_object$STUSPS, 
                           ~{
                             discharge_sites <- whatNWISsites(stateCd = .x, parameterCd = "00060") %>%
                               filter(site_tp_cd == 'ST')
                             
                             # Only use gages under 1500 square kilometers (as defined by the USGS):
                             small_enough <- readNWISsite(discharge_sites$site_no) %>%  
                               # mutate(drain_area_km = drain_area_va *  2.58999) %>%  
                               # filter(drain_area_km <= 1500) 
                               
                               return(small_enough)
                           }
)

nwis_sites <- bind_rows(nwis_sites_by_state) %>%
  distinct(.) %>%
  st_as_sf(coords = c("dec_long_va", "dec_lat_va"), crs = 4269)

mapview(nwis_sites)
```

The missing sites seem to mostly be CO gages, though eight have site_no nomenclature. It's likely these were picked up by CO...

```{r}
missing_abby <- abby %>%
  anti_join(., st_drop_geometry(nwis_sites), by = c("site_no"))
```

Next, filter to only USGS sites with drainage areas under 1500 sqkm (plus some wiggle) based on USGS WS size:

```{r}
small_enough <- nwis_sites %>%
  mutate(drain_area_km = drain_area_va *  2.58999) %>%  
  # 15k plus some wiggle room. Or, we can remove...
  filter(drain_area_km <= 1550)

too_big <- nwis_sites %>%
  mutate(drain_area_km = drain_area_va *  2.58999) %>%  
  # 15k plus some wiggle room. Or, we can remove...
  filter(drain_area_km > 1550)
```

Subset to USGS gages with data since at least the 2000 water year:

```{r}
gage_meta_1 <- dataRetrieval::whatNWISdata(siteNumber = small_enough$site_no[1:1500], parameterCd = "00060")

gage_meta_2 <- dataRetrieval::whatNWISdata(siteNumber = small_enough$site_no[1501:length(small_enough$site_no)], parameterCd = "00060")

gage_meta <- bind_rows(gage_meta_1, gage_meta_2) %>% distinct()

rm(gage_meta_1, gage_meta_2)

tables <- rvest::read_html('https://help.waterdata.usgs.gov/parameter_cd?group_cd=%') %>%
  rvest::html_nodes('table') %>%
  rvest::html_table()

pcodes <- tables[[1]] %>% 
  janitor::clean_names() %>% 
  dplyr::mutate(parm_cd = stringr::str_pad(as.character(parameter_code), 5, pad = "0"))

inventory <- gage_meta %>%
  dplyr::left_join(pcodes,by="parm_cd") %>%
  dplyr::select(c(site_name = station_nm,
                  site_no,
                  data_type_cd,
                  site_type_cd = site_tp_cd,
                  n_obs=count_nu,
                  begin_date,
                  end_date,
                  parameter=parameter_name_description,
                  code=parm_cd))

site_url <- 'https://maps.waterdata.usgs.gov/mapper/help/sitetype.html'

table <- rvest::read_html(site_url) %>%
  rvest::html_nodes('table') %>%
  rvest::html_table() #%>%

table <- rbind(table[[1]],table[[2]],table[[3]],table[[4]],table[[5]]) %>%
  dplyr::select(site_type_cd = 1,
                site_type = 2)

inventory <- left_join(inventory,table,by='site_type_cd') %>%
  mutate(data_type=case_when(data_type_cd=="dv" ~ "Daily",
                             data_type_cd=="uv" ~ "Unit",
                             data_type_cd=="qw" ~ "Water Quality",
                             data_type_cd=="gw" ~ "Groundwater Levels",
                             data_type_cd=="iv" ~ "Unit",
                             data_type_cd=="sv" ~ "Site Visits",
                             data_type_cd=="pk" ~ "Peak Measurements",
                             data_type_cd=="ad" ~ "USGS Annual Water Data Report",
                             data_type_cd=="aw" ~ "Active Groundwater Level Network",
                             data_type_cd=="id" ~ "Historic Instantaneous"))

new_data_gages <- inventory %>%
  filter(year(begin_date) <= "1999",
         year(end_date) > "1999",
         data_type == "Daily")

# Only keep gages with data during and after 1999:
nwis_sites_data_record <- small_enough %>% 
  filter(site_no %in% new_data_gages$site_no)
```

Grab CO DWR gages...

```{r}
# Pull all of Abby's sites:
abby_sites <- httr::GET(url = "https://dwr.state.co.us/Rest/GET/api/v2/surfacewater/surfacewaterstations/?format=json&fields=stationNum%2Cabbrev%2CusgsSiteId%2CstationName%2CutmX%2CutmY%2Clatitude%2Clongitude%2CstartDate%2CendDate%2CmeasUnit") %>%
  httr::content(., as = "text", encoding = "UTF-8") %>%
  jsonlite::fromJSON() %>%
  .[["ResultList"]] %>%
  # Due to naming weirdness... grab any/all combos listed in Abby's dataframe:
  filter((!is.na(stationName) & stationName %in% abby$statname) |
           (!is.na(abbrev) & abbrev %in% abby$site_no) |
           (!is.na(abbrev) & abbrev %in% abby$co_no |
              # rename from "BIGWECO" to "BIGWESCO"
              abbrev == "BIGWESCO")) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4269) %>%
  mutate(site_no = ifelse(is.na(abbrev), usgsSiteId, abbrev)) %>%
  select(site_no,
         co_usgs = usgsSiteId,
         station_nm = stationName)

# are they all accounted for?
nrow(abby_sites) == nrow(abby_raw)
# ... yes!

# Pull ALL CDWR sites, that meet our data requirements of 1999-2024
# that are also stream gages: 
cdwr_sites <- httr::GET(url = "https://dwr.state.co.us/Rest/GET/api/v2/surfacewater/surfacewaterstations/?format=json&fields=stationNum%2Cabbrev%2CusgsSiteId%2CstationName%2CutmX%2CutmY%2Clatitude%2Clongitude%2CstartDate%2CendDate%2CmeasUnit") %>%
  httr::content(., as = "text", encoding = "UTF-8") %>%
  jsonlite::fromJSON() %>%
  .[["ResultList"]] %>%
  mutate(site_no = ifelse(is.na(abbrev), usgsSiteId, abbrev)) %>%
  filter(!is.na(longitude) & !is.na(latitude)) %>%
  filter(year(endDate) > "1999",
         year(startDate) <= "1999") %>%
  # # Obnoxiously station type cannot be accessed on API only GUI
  filter(abbrev %in% c(read_csv("data/cdwr.csv") %>%.$Abbrev)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4269) %>%
  select(site_no,
         co_usgs = usgsSiteId,
         station_nm = stationName) %>%
  # ensure Abby's sites are in here
  bind_rows(abby_sites) %>%
  # remove dupes caused by adding Abby's in:
  distinct()

# Bind CO DWR and USGS sites together
all_sites <- nwis_sites_data_record %>% bind_rows(cdwr_sites) %>%
  # # For future tracking with the NLDI:
  mutate(site_pretty = ifelse(is.na(co_usgs), paste0("USGS-", site_no),
                            paste0("USGS-", co_usgs))) %>%
  select(-co_usgs) %>%
  distinct()

# Some USGS gages have been handed off to CDWR - and, many (or maybe even all?) USGS gages are listed in CDWR
# Here I'm removing those dupes, keeping the CDWR "version"
dupe_test <- all_sites %>%
  group_by(site_pretty) %>%
  mutate(count = n()) %>%
  filter(count > 1) %>%
  group_by(site_no, station_nm) %>%
  mutate(count = n()) %>%
  filter(count == 1)

all_sites <- all_sites %>%
  mutate(agency_cd = ifelse(is.na(agency_cd), "CDWR", agency_cd)) %>%
  arrange(agency_cd) %>%
  distinct(site_pretty, .keep_all = TRUE) %>%
  select(agency_cd, site_no, station_nm, site_pretty)
```

Filter by data availability, 2000 - 2024 wyears with 75% approved data:

```{r, eval = FALSE}
# Define the start and end dates
start_date <- as.Date("2000-10-01")
end_date <- as.Date("2024-09-30")
days <- as.numeric(end_date - start_date) + 1

usgs_only <- all_sites %>%
  filter(!is.na(as.numeric(str_extract(site_pretty, "(?<=-).*"))))

enough_data <- vector("list", length = nrow(usgs_only))

for(i in 1:nrow(usgs_only)){
  
  enough_data[[i]] <- tibble(site_no = usgs_only[i,]$site_no,
                             n_days = 0)
  
  try(data <- readNWISdv(siteNumbers = str_extract(usgs_only[i,]$site_pretty, "(?<=-).*"), 
                         startDate = "1999-10-01", endDate = "2024-09-30",
                         parameterCd = "00060") %>%
        filter(X_00060_00003_cd %in% c("A", "A e")) %>%
        distinct(),
      silent = TRUE)
  
  try(enough_data[[i]] <- tibble(site_no = usgs_only[i,]$site_no,
                                 n_days = nrow(data)),
      silent = TRUE)
  
  message(usgs_only[i,]$site_no)
  
}

enough <- enough_data %>% bind_rows() %>%
  filter(n_days >= days * 0.75)
saveRDS(enough, "data/katie/enough_data.RDS")
```


```{r}
enough <- readRDS("data/katie/enough_data.RDS")
all_sites <- all_sites %>%
  # I'm not currently filtering out the CDWR sites by data availability. 
  filter(agency_cd == "CDWR" | site_no %in% enough$site_no)
```


Now, let's find additional gages with enough SEASONAL data, which we will define
as >= 90% data availability for all months from May-Sep from 2000-2024 water years:

```{r, eval = FALSE}
days <- 153 # number of total days in May, June, July, August, and September
enough_seasonal_data <- vector("list", length = nrow(usgs_only))

for(i in 1:nrow(usgs_only)){
  
  enough_seasonal_data[[i]] <- tibble(site_no = usgs_only[i,]$site_no,
                                      n_summer_days = 0)
  
  try(data <- readNWISdv(siteNumbers = str_extract(usgs_only[4,]$site_pretty, "(?<=-).*"), 
                         startDate = "1999-10-01", endDate = "2024-09-30",
                         parameterCd = "00060") %>%
        filter(X_00060_00003_cd %in% c("A", "A e")) %>%
        distinct() %>%
        filter(month(Date) %in% c(5,6,7,8,9)),
      silent = TRUE)
  
  try(enough_seasonal_data[[i]] <- tibble(site_no = usgs_only[i,]$site_no,
                                          n_summer_days = nrow(data)),
      silent = TRUE)
  
  message(usgs_only[i,]$site_no)
  
}

enough_seasonal <- enough_seasonal_data %>%
  bind_rows() %>%
  filter(n_summer_days >= days * 0.90)

saveRDS(enough_seasonal, "data/katie/enough_seasonal_data.RDS")
```

```{r}
enough_seasonal <- readRDS("data/katie/enough_seasonal_data.RDS")
```




Delineate stream gage watersheds

For this analysis, we are relying on NHDPlus Version 2. We are able to delineate a stream gage's upstream contributing area (i.e., its watershed) by leveraging the NHD's network index that, for every stream feature in the NHD, identifies all other stream features upstream of it. So, our first task is to find out which NHD stream feature each gage is associated with. All stream features are given a unique ID, called a comid. Every stream feature also has an associated "catchment", or direct contributing area, with the same comid. So here, we are identifying the comid for the stream feature each gage is associated with.

```{r}
nwis_sites$comid <- NA # attempt 'get_nldi_feature()' first
nwis_sites$comid_coords <- NA # if that doesn't work for all gages, do 'discover_nhdplus_id()'

# first try to get comid using nldi ("verified" correct comid - or at least what USGS says it is)
for(i in 1:nrow(nwis_sites)){
  try(nwis_sites$comid[i] <- get_nldi_feature(list("featureSource" = "nwissite", featureID = nwis_sites[i,]$site_pretty))$comid, silent = T)
}

# get the comid using the weirdos' coordinates instead of their gage name
for(i in 1:nrow(nwis_sites)){
  nwis_sites$comid_coords[i] <- discover_nhdplus_id(nwis_sites[i,])
}

nwis_sites_w_states <- nwis_sites %>%
  st_join(tigris::states() %>% select(STUSPS))

# Ones where the USGS says they fall on a comid they don't technically fall on.
weirdos <- nwis_sites_w_states %>% filter(comid_coords != comid)

mapview(weirdos) + mapview(get_nhdplus(comid = weirdos$comid), color = "blue", layer.name = "By NLDI") + mapview(get_nhdplus(comid = weirdos$comid_coords), color = "red", layer.name = "By coordinates")

# I went through and manually identified the appropriate comids for all of these:
comid_right <- read_csv("data/katie/nldi_vs_nhdtools.csv")

nwis_sites_data_record_nldi <- nwis_sites_w_states %>%
  left_join(., comid_right, by = "site_no") %>%
  # Replace "bad" comids with manually verified comids:
  mutate(comid_new = ifelse(is.na(comid_right), comid_coords, comid_right)) %>%
  select(site_no, station_nm, comid = comid_new) %>%
  mutate(comid = as.numeric(comid)) %>%
  # lil' cleaning of a particular comid I screwed up
  mutate(comid = ifelse(site_no == "09292000", 944060076, comid))

# back it up:
saveRDS(nwis_sites_data_record_nldi, 'data/katie/nwis_sites_data_record_nldi.RDS')
```  

```{r}
nwis_sites_data_record_nldi <- readRDS('data/katie/nwis_sites_data_record_nldi.RDS') %>%
  # in case dupes were made on accident:
  distinct(site_no, station_nm, .keep_all = TRUE)
```

Delineate each gage's watershed

Now that we have a list of our gages and their associated NHDPlus V2 stream features, we can use the NHD indexing to "crawl" upstream of each gage's flowline, then grab each flowline's catchment, and lastly dissolve those catchments into a single polygon the represents the gage's upstream contributing area (i.e., its watershed):

```{r}
# load in the NHD as a table. This table lists all COMIDs in CONUS and allows you to "navigate" the NHD.
nhd <- read_csv("data/nhd_flow_network.csv")

# function to delineate each gage's watershed:
watershed_delineator <- function(site_list){
  # these are watersheds that are contained within a catchment (very small). require
  # a different workflow:
  if (site_list %in% c("SPACRECO", "10172200", "08329700", "SCHFLMCO")){
    
    # Filter to get a specific site from the dataset and transform to WGS84 coordinate system
    site <- nwis_sites_data_record_nldi %>%
      filter(site_no == site_list) %>%
      st_transform(4326)
    
    # Get the NHDPlus flowline near the site and create a 30m buffer around it
    # This is considered a "danger zone" to avoid placing points on the flowline
    flowline_danger_zone <- get_nhdplus(AOI = site, realization = "flowline") %>%
      st_buffer(30)
    
    # Create a 35m buffer around the site location
    site_buffer <- site %>%
      st_buffer(35) 
    
    # Extract points from the buffer boundary while avoiding the flowline
    boundary_points <- site_buffer %>%
      st_boundary() %>%      # Extract the boundary (a LINESTRING)
      st_cast("POINT") %>%   # Convert the LINESTRING to individual POINTs
      mutate(point_id = row_number()) %>%  # Add an ID for each point
      st_difference(., flowline_danger_zone) %>%  # Remove points that fall within the flowline buffer
      # Reduce the number of points by taking only every 50th point
      filter(point_id %% 50 == 0) %>%
      bind_rows(site)  # Add the original site point to the collection
    
    # Initialize an empty list to store catchment splits
    splits <- vector("list")
    
    # Loop through each boundary point to get split catchments
    for(i in 1:nrow(boundary_points)){
      
      # Get the split catchment for each point
      # We're looking for areas without a catchmentID (NA values)
      splits[[i]] <- get_split_catchment(point = st_as_sfc(boundary_points[i,])) %>%
        filter(is.na(catchmentID)) %>%
        mutate(area = as.numeric(st_area(.)))  # Calculate the area
    }
    
    # Combine all splits and select the largest one
    splits <- bind_rows(splits) %>%
      filter(as.numeric(area) == max(as.numeric(area))) %>%  # Keep only the largest area
      select(-catchmentID,-id) %>%  # Remove unnecessary columns
      mutate(ws_flag = "SMALL") %>%  # Add a flag indicating this is a small watershed
      # Add site information and initialize area metrics as NA
      dplyr::mutate(site_no = site$site_no,
                    comid = site$comid,
                    area_original_m2 = NA,
                    area_updated_m2 = NA,
                    area_dif_m2 = NA)
    # back it up:
    saveRDS(splits, paste0("data/katie/watersheds/", site$site_no, ".RDS"))
    
    message(paste0(site$station_nm, " delineated!"))
    
  } else {
    
    # filter our master list to just the gage we are iterating over
    site <- nwis_sites_data_record_nldi %>%
      filter(site_no == site_list) %>%
      st_transform(4326)
    
    raindrop <- get_raindrop_trace(point = st_as_sfc(site), direction = "down")
    
    flowline <- get_nhdplus(comid = site$comid)#st_buffer(site, 200)) 
    
    if(raindrop[1,3]$comid == site$comid) {
      
      nearest_point <- st_as_sf(data.frame(x = raindrop$intersection_point[[1]][1],
                                           y = raindrop$intersection_point[[1]][2]),
                                coords = c("x", "y"),
                                crs = st_crs(raindrop)) %>%
        st_as_sfc()
      
      better_termination <- get_split_catchment(nearest_point, upstream = F) %>%
        filter(is.na(catchmentID)) %>%
        mutate(featureid=site$comid)
      
      
    } else {
      message("One of the funky ones")
      nearest_points <- st_nearest_points(site, flowline, pairwise = T) %>%
        st_cast(., "POINT") %>%
        st_as_sf() %>%
        st_make_valid()  %>%
        mutate(distance = as.numeric(st_distance(., site))) %>%
        distinct() %>%
        arrange(distance) 
      
      nearest_point_1 <- nearest_points %>%
        .[1,] %>%
        st_as_sfc()
      
      nearest_point_2 <- nearest_points %>%
        .[2,] %>%
        st_as_sfc()
      
      better_termination_1 <- get_split_catchment(nearest_point_1, upstream = F) %>%
        filter(is.na(catchmentID)) 
      
      better_termination <- get_split_catchment(nearest_point_2, upstream = F) %>%
        filter(is.na(catchmentID)) %>%
        bind_rows(better_termination_1) %>%
        summarize() %>%
        mutate(featureid = as.numeric(flowline$comid))
      
      # Weird ones that require more work:
      if(site$site_no == "VALBAYCO"){
        better_termination <- better_termination %>%
          mutate(featureid = 17034207)
      } else if(site$site_no == "LVNGEOCO"){
        better_termination <- better_termination %>%
          mutate(featureid = 2885290)
      }
    }
    
    # use get_UT to list all comids that are upstream of our gage using the comid the
    # gage falls on:
    upstream <- nhdplusTools::get_UT(nhd, better_termination$featureid)
    
    # These ones are weird braided channels that the NHD doesn't
    # work super well with:
    if(site$site_no == "NORLASCO"){
      more_upstream <- get_UT(nhd, "17876457")
      upstream <- c(upstream, more_upstream)  
    } else if(site$site_no == "NORCONCO"){
      more_upstream <- get_UT(nhd, "17875505")
      upstream <- c(upstream, more_upstream)  
    } else if(site$site_no == "TOMGUNCO"){
      more_upstream <- get_UT(nhd, "9772879")
      upstream <- c(upstream, more_upstream)  
    }
    
    
    # grab all the catchments associated with the upstream comids:
    nhd_catch <- nhdplusTools::get_nhdplus(comid = upstream,
                                           realization = 'catchment') 
    # another weird one where the NHD is broken and the catchment comid and nhd comid
    # aren't the same:
    if(site$site_no == "06799445"){
      nhd_catch <- nhdplusTools::get_nhdplus(AOI = site,
                                             realization = 'catchment') %>%
        mutate(featureid = flowline$comid) %>%
        bind_rows(nhd_catch)
    }
    
    site_catch <- nhd_catch %>%
      filter(featureid == better_termination$featureid) %>%
      sf::st_area()
    
    split_catch <- better_termination %>%
      sf::st_area()
    
    # Weirdos:
    if(site$site_no %in%c("WFKMOUCO","NORCONCO")){
      split_catch <- nhd_catch %>%
        filter(featureid == better_termination$featureid) %>%
        sf::st_area()
      better_termination <- nhd_catch %>%
        filter(featureid == better_termination$featureid)
    }
    
    dif <- as.numeric(site_catch) - as.numeric(split_catch)
    
    if(raindrop[1,3]$comid != site$comid && dif < 0) {
      split_catch <- 0 
      dif <- as.numeric(site_catch) - as.numeric(split_catch)
    }
    
    if(raindrop[1,3]$comid == site$comid && dif < 0) {
      split_catch <- site_catch 
      dif <- as.numeric(site_catch) - as.numeric(split_catch)
    }
    
    nhd_watershed <- nhd_catch %>%
      # remove dupes (precautionary step, not likely necessary)
      dplyr::distinct(featureid, .keep_all=TRUE) %>%
      dplyr::filter(featureid != better_termination$featureid) %>%
      dplyr::bind_rows(., better_termination) %>%
      st_make_valid() %>%
      # "dissolve" all the catchments into a single polygon
      dplyr::summarize() %>%
      # remove weird hole by-products that exist if the catchment boundaries don't
      # line up perfectly:
      nngeo::st_remove_holes() %>%
      # tack on the site name and comid to the watershed
      dplyr::mutate(site_no = site$site_no,
                    comid = site$comid,
                    area_original_m2 = as.numeric(site_catch),
                    area_updated_m2 = as.numeric(split_catch),
                    area_dif_m2 = dif)
    
    # mapview(nhd_watershed) + site + flowline
    
    # back it up:
    saveRDS(nhd_watershed, paste0("data/katie/watersheds/", site$site_no, ".RDS"))
    
    message(paste0(site$station_nm, " delineated!"))
  }
}

# Create a vector of nwis sites to iterate over
nwis_sites_data_record_nldi %>% 
  pull(site_no) %>%
  #... then delineate each site's watershed:
  walk(~try(watershed_delineator(.))) 
```

```{r}
watersheds <- list.files("data/katie/watersheds/", full.names = TRUE) %>%
  map_dfr(~readRDS(.) %>% mutate(comid = as.numeric(comid))) %>%
  st_make_valid() %>%
  # add states back in:
  inner_join(nwis_sites %>% st_join(tigris::states() %>% 
                                      select(STUSPS)) %>% 
               select(agency_cd, site_pretty, site_no, STUSPS) %>% 
               st_drop_geometry(.) , by = "site_no") %>%
  # confirm watersheds are included in the upstream list:
  filter(site_no %in% nwis_sites_data_record_nldi$site_no,
         # These are massive watersheds (well beyond 15k sq.km.:)
         !site_no %in% c("PURNINCO", "PLACROCO", "PLAJURCO", 
                         "ARKJMRCO", "ARKHOLCO", "COLUTACO", 
                         "PLAMORCO", "PLABALCO", "ARKLAMCO",
                         "GRNJENUT", "ARKROCCO", "ONEJURCO",
                         "ARKCOOKS", "ARKGARKS", "ARKSYRKS", 
                         "RIOALBNM", "PLAROSNE", "RGFSANNM", 
                         "SMFRIONM", "RIOELENM", "ARKGRACO")) %>%
  mutate(total_area_km2 = as.numeric(st_area(.)) / 1000000) %>%
  filter(total_area_km2 <= 1500) %>%
  mutate(old_site_no =  str_extract(site_pretty, "(?<=-).*")) %>%
  arrange(agency_cd) %>%
  distinct(old_site_no, .keep_all = TRUE) 

flowlines <- vector("list", length = nrow(watersheds))

for(i in 1:nrow(watersheds)){
  
  site <- watersheds[i,] %>% st_buffer(1000)
  
  flowlines[[i]] <- try(nhdplusTools::get_nhdplus(AOI = site,
                                                  realization = 'flowline',
                                                  t_srs = 4269) %>%
                          mutate(old_site_no = site$old_site_no,
                                 site_no = site$site_no) %>%
                          mutate(across(where(~ !inherits(.x, "sfc")), as.character)))
  
}

flowlines <- flowlines %>%
  keep(~!is.character(.)) %>%
  bind_rows() %>%
  select(site_no, old_site_no, comid)

sites <- nwis_sites %>%
  mutate(old_site_no =  str_extract(site_pretty, "(?<=-).*")) %>%
  filter(site_no %in% watersheds$site_no) %>%
  select(agency_cd, site_no, old_site_no, station_nm)

# Look at them:
create_individual_site_maps(test_watersheds = watersheds,
                            test_sites = sites, 
                            flowlines = flowlines)

saveRDS(watersheds, "data/katie/katies_watersheds.RDS")
```

```{r}
stephanie_list <- read_csv('data/katie/stephanie_watershed_list.csv') %>%
  mutate(site_no = case_when(
    # Check if the first character is 6, 7, 8, or 9
    str_sub(site_no, 1, 1) %in% c("6", "7", "8", "9") ~ paste0("0", site_no),
    # Otherwise, keep the original site_no
    TRUE ~ site_no
  )) %>%
  select(old_site_no = site_no, modif_type, class) %>%
  mutate(checked = "SK",
         quality = "model quality")

watersheds_old <- st_read("data/katie/watersheds_screened.shp") %>%
  select(old_site_no = site_no) %>%
  filter(!old_site_no %in% stephanie_list$old_site_no) %>%
  mutate(checked = "SK",
         quality = "not model quality") %>%
  st_drop_geometry() %>%
  bind_rows(stephanie_list)


missing_abby <- abby %>%
  filter(!site_no %in% watersheds$site_no & !co_no %in% watersheds$site_no &
           !site_no %in% watersheds$old_site_no & !co_no %in% watersheds$old_site_no)

my_watersheds <- readRDS("data/katie/katies_watersheds.RDS") %>%
  left_join(st_drop_geometry(watersheds_old), by = c("old_site_no" = "old_site_no")) %>%
  select(site_no, old_site_no, checked, quality, modif_type, class) %>%
  arrange(checked)

st_write(my_watersheds, "data/katie/for_stephanie/all_watersheds.shp")
st_write(flowlines, "data/katie/for_stephanie/all_flowlines.shp")
st_write(sites, "data/katie/for_stephanie/all_sites.shp")
```